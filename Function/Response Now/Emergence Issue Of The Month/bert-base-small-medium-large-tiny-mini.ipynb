{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483}],"dockerImageVersionId":30192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-04T13:02:00.910236Z","iopub.execute_input":"2024-01-04T13:02:00.910534Z","iopub.status.idle":"2024-01-04T13:02:00.919421Z","shell.execute_reply.started":"2024-01-04T13:02:00.910501Z","shell.execute_reply":"2024-01-04T13:02:00.918651Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"/kaggle/input/sms-spam-collection-dataset/spam.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.models import Model\n\nimport transformers\nfrom transformers import BertTokenizer, TFBertModel\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:00.920862Z","iopub.execute_input":"2024-01-04T13:02:00.921130Z","iopub.status.idle":"2024-01-04T13:02:00.934159Z","shell.execute_reply.started":"2024-01-04T13:02:00.921101Z","shell.execute_reply":"2024-01-04T13:02:00.933455Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding=\"ISO-8859-1\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:00.935053Z","iopub.execute_input":"2024-01-04T13:02:00.935253Z","iopub.status.idle":"2024-01-04T13:02:00.956878Z","shell.execute_reply.started":"2024-01-04T13:02:00.935226Z","shell.execute_reply":"2024-01-04T13:02:00.956164Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:00.958012Z","iopub.execute_input":"2024-01-04T13:02:00.958298Z","iopub.status.idle":"2024-01-04T13:02:00.969581Z","shell.execute_reply.started":"2024-01-04T13:02:00.958259Z","shell.execute_reply":"2024-01-04T13:02:00.968786Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"     v1                                                 v2 Unnamed: 2  \\\n0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n1   ham                      Ok lar... Joking wif u oni...        NaN   \n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3   ham  U dun say so early hor... U c already then say...        NaN   \n4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n\n  Unnamed: 3 Unnamed: 4  \n0        NaN        NaN  \n1        NaN        NaN  \n2        NaN        NaN  \n3        NaN        NaN  \n4        NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.drop(['Unnamed: 2', 'Unnamed: 3' ,'Unnamed: 4'], axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:00.971441Z","iopub.execute_input":"2024-01-04T13:02:00.971645Z","iopub.status.idle":"2024-01-04T13:02:00.978844Z","shell.execute_reply.started":"2024-01-04T13:02:00.971619Z","shell.execute_reply":"2024-01-04T13:02:00.977918Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"df.rename(columns={'v1': 'Class', 'v2': 'Text'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:00.979930Z","iopub.execute_input":"2024-01-04T13:02:00.980142Z","iopub.status.idle":"2024-01-04T13:02:00.988451Z","shell.execute_reply.started":"2024-01-04T13:02:00.980114Z","shell.execute_reply":"2024-01-04T13:02:00.987800Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"df['Class'] = df['Class'].map({'ham':0, 'spam':1})\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:00.989326Z","iopub.execute_input":"2024-01-04T13:02:00.989523Z","iopub.status.idle":"2024-01-04T13:02:01.004166Z","shell.execute_reply.started":"2024-01-04T13:02:00.989498Z","shell.execute_reply":"2024-01-04T13:02:01.003465Z"},"trusted":true},"execution_count":109,"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"   Class                                               Text\n0      0  Go until jurong point, crazy.. Available only ...\n1      0                      Ok lar... Joking wif u oni...\n2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n3      0  U dun say so early hor... U c already then say...\n4      0  Nah I don't think he goes to usf, he lives aro...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Now we can see this is imbalanced target \ndf['Class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:01.005018Z","iopub.execute_input":"2024-01-04T13:02:01.005219Z","iopub.status.idle":"2024-01-04T13:02:01.015233Z","shell.execute_reply.started":"2024-01-04T13:02:01.005193Z","shell.execute_reply":"2024-01-04T13:02:01.014552Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"0    4825\n1     747\nName: Class, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"sns.set(style = \"darkgrid\" , font_scale = 1.2)\nsns.countplot(df.Class).set_title(\"Number of ham and spam messages\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:01.016222Z","iopub.execute_input":"2024-01-04T13:02:01.016469Z","iopub.status.idle":"2024-01-04T13:02:01.187240Z","shell.execute_reply.started":"2024-01-04T13:02:01.016434Z","shell.execute_reply":"2024-01-04T13:02:01.186385Z"},"trusted":true},"execution_count":111,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAZoAAAEhCAYAAABGC2bVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAs40lEQVR4nO3deVhU5d8/8DeDbMmiuD2iBkLCiOy4JyoqmIY+Zg+GKKZii1SKpiKU+sVCe8LUiCSLzFz4mghiqF/ycs1UVFxjyyXQFAkExSHZZO7fH/44jyMuoJxB8P26Li6d+77POZ8zwLyZc87cR0cIIUBERCQTRWMXQEREzRuDhoiIZMWgISIiWTFoiIhIVgwaIiKSFYOGiIhkxaChx7py5Qrs7OyQlpbW2KVoKC0txXvvvQd3d3fY2dnhypUrtcYcPXoUdnZ2yM/Pb4QKtS8gIAAfffRRY5dBpIFB84ybP38+7Ozs8Pnnn2u05+fnw87ODkePHm2kyhpfXFwcTp8+jbi4OPz222/o2LFjY5dERA/AoGkCDAwMsH79ely9erWxS2lwVVVVT7zspUuX8NJLL8HOzg7t2rWDrq5uA1ZGRA2FQdMEuLq6QqlUYvny5Q8d87DDW15eXvjqq6+kx3Z2dli/fj2Cg4Ph4uKCwYMHIyUlBSqVCh9++CFcXV0xdOhQ/PLLL7W2cfXqVbz55ptwcnLC0KFDsWPHDo3+69evY/78+ejbty9cXV3h5+eH48ePS/01h7H279+P8ePHw9HREfHx8Q/cn6qqKixbtgweHh5wcHDAyJEjkZycLPUPGTIEW7ZsQWpqKuzs7BAQEPDI5/DixYuYMGECnJ2dMXLkSBw4cECjf8WKFRgxYgScnZ0xaNAgLFy4ECqVSupPTEyEvb09UlNTMWrUKDg5OSEgIAB///03jh8/jjFjxsDFxQWTJ0/G33///chakpOT4evrC3d3d/Tp0wdvv/02cnJypP6a7+XOnTvxzjvvwNnZGUOHDkViYqLGeq5evYrAwEA4OTlh0KBBWL9+/SO3W/O8Ll26FAMHDoSDgwMGDBiAWbNmSf3z58/H5MmTsXbtWnh4eMDZ2RkzZszAzZs3pTEZGRmYNm0a+vXrB1dXV7z++uv49ddfNbYzZMgQrFy5EosWLULPnj3Rr18/bNiwAZWVlfjkk0/Qq1cveHh4YMOGDY+s92me90OHDsHPzw9OTk7w8PBAaGgobty4IfWfP38egYGB6NmzJ1xcXDBixAgkJSVJ/fHx8RgxYgQcHR3Ru3dvTJgwQToEW1JSgjlz5mDw4MFwcnLC8OHDsWbNGtw70Yparcby5cul34dZs2Zh7dq1sLe3b9A6mwRBz7SQkBDx5ptviuPHjws7Oztx9uxZIYQQ165dE7a2tiI1NVUIIcRff/0lbG1txfHjxzWWHzZsmIiKipIe29raiv79+4vExESRm5srFi1aJBwdHUVgYKBISEgQubm5YvHixcLZ2VkUFxdrrPvll18W27ZtExcvXhTLly8XSqVSZGRkCCGEKCsrEyNGjBDvv/++OHv2rMjNzRWrVq0SPXr0EBcuXBBCCJGamipsbW3F8OHDxZ49e8Tly5fFtWvXHrjfn332mejdu7fYuXOn+PPPP0VMTIyws7MThw8fFkIIUVRUJGbOnCn8/f1FQUGBuHHjxgPXU7PNUaNGiQMHDoicnBwxf/584erqKm7evCmN+/rrr8Xx48fFX3/9JQ4fPiyGDx8u5s2bJ/UnJCQIOzs7MXHiRHH69GmRnp4uvLy8xPjx48XEiRPFqVOnRGZmphg+fLiYOXPmI7+nW7ZsEXv27BGXLl0SGRkZ4p133hFeXl6ioqJC4/keMmSI2LFjh8jNzRVffPGF6N69u/jzzz+FEEKo1WoxZswYMXbsWHH69GmRmZkpJk+eLFxdXUVYWNhDt71mzRrh4eEhUlNTxdWrV8WZM2fEDz/8IPWHhIQIV1dX8c4774js7GyRmpoqvLy8RFBQkMZzmpCQIM6dOyf+/PNPsXz5ctGjRw+pNiGE8PT0FO7u7mLNmjUiNzdXfP3118LW1lZMmzZNavvmm2+EnZ2dOH/+/EPrfdLn/fDhw8LJyUmsW7dO5OTkiDNnzoiJEyeKCRMmCLVaLYQQwsfHR8yePVucP39eXL58Wezfv1/s3btXCCHE77//Lrp37y62bt0qrly5IrKzs8XmzZuln9eCggKxevVqkZ6eLi5fviySkpKEi4uL2LJli8Zz7eLiIrZu3SpycnLEmjVrRK9evUT37t0brM6mgkHzjKsJGiGECAoKEhMnThRCPF3QfPrpp9LjoqIiYWtrKxYvXiy13bx5U9ja2ko/zDXrXrFihca633jjDTFnzhwhxN0XBA8PD1FVVaUxJiAgQNpezYv+1q1bH7nPt2/fFj169BAbNmzQaA8KChIBAQEPfG4epmabv/zyi9RWWFgobG1txa+//vrQ5Xbt2iV69Oghqqurpf2ztbUVmZmZ0pjvvvtO2Nrait9//11q++GHH0Tv3r0fWdP9bty4IWxtbUVaWpoQ4v+e7zVr1khj7ty5I1xcXMS///1vIYQQhw4dEra2thov7kVFRcLR0fGRQfPJJ5+IgIAA6UXsfiEhIcLFxUXcunVLajt48KCwtbUVubm5D13vqFGjxKpVq6THnp6eYvr06dLj6upqKcDubevZs6dYv379Q9f7pM/7xIkTRWRkpMa6rl69qrEuNzc3kZCQ8MDt7tq1S7i5uQmVSvXQ2u73ySefiMmTJ0uPBwwYUOt3Jjg4WCNonrbOpqJFY7+jorqbM2cOfHx8sGfPHvTo0eOJ16NUKqX/m5ubQ1dXF3Z2dlKbmZkZ9PT0UFRUpLGcq6trrcepqakAgN9//x3Xr19Hr169NMZUVlbC0NBQo83JyemR9V26dAlVVVW11tWrVy98++23j9m7B+vevbv0/7Zt20JXV1dj/3bt2oUff/wRly5dwj///AO1Wo2qqioUFhaiQ4cOAAAdHR3Y2tpqrAeAxnPXtm1b3Lx5E9XV1Q89Z5SVlYXo6GhkZWVpHCLJy8uDu7u79Pje75Ouri7atGmD69evAwAuXLiA1q1bo2vXrtIYc3NzjccP8vrrr2PKlCnw8vJC//798fLLL8PT0xP6+vrSGBsbG5iYmEiP3dzcpG1aWlqiuLgYUVFRSE1NxfXr11FdXY2Kigrk5eVpbOve+hUKBczNzTWeq5q2+3/O7vckz/vvv/+O06dPY+PGjbXWl5ubi+7du2Pq1Kn4+OOPsXXrVvTu3RtDhgyRfq/69++PLl26YOjQoejfvz/69u0LLy8vmJubA7h7WCw2NhY7duxAfn4+KisrUVVVhU6dOgEAVCoVCgoK4OLiorFtFxcXjcPST1tnU8GgaUK6du2KN954A8uWLcN3332n0adQPPh02507d2q1tWhR+9t+f5uOjo7G8ebHUavVsLGxQXR0dK2++4PmhRdeqPN6G4qenl6tNrVaDQA4c+YMZs6cibfffhvz5s2Dqakpzpw5g5CQEI2LFRQKhUZ46Ojo1Fp3TdvDnruysjJMnToV7u7uWLp0qfSi+eqrr9a6MOL+muv7PXmQ7t27Y8+ePTh8+DCOHj2KiIgIfPnll9i8eTOMjY3rtI758+fj2rVrmDt3Ljp37gxDQ0PMmjWrVv0P+pl6UFvN9+FhnuR5V6vVeOutt/Df//3ftdZX85y/9957GD16NH799VccPXoUq1evRmBgIGbNmoWWLVsiISEBJ0+exOHDh7Fp0yZERkZi7dq1cHBwwJo1a7B69WqEhobC3t4eLVu2xNq1a2ud+6up62Gets6mghcDNDHvv/8+CgoKsHnzZo32mr+0CgoKpLaioqLHnpiuj9OnT2s8PnXqFGxsbAAADg4O+Ouvv2BsbAxLS0uNr5p3BHVlaWkJfX19jQsJAOD48ePo1q3bU+3Dg5w4cQKtW7fGrFmz4OzsjK5du8r2uZuLFy+iuLgYs2bNQp8+fWBjY4OSkpJ6B8hLL72EGzduIDc3V2orLi7WuKjgYVq2bAkvLy98/PHHSEhIwMWLF3Hs2DGNGktLS6XHp06dkrYJ3P0+jB8/HkOHDpWu+HvQZ5gak4ODg/QO7P6vli1bSuO6dOmCCRMmICoqCjNmzMCmTZukPl1dXfTq1QszZ85EYmIi2rVrh+3btwMA0tLS4OHhgf/5n/+Bvb09LC0tcenSJWlZExMTtG/fXnruapw5c6bB62wK+I6miTE3N8fbb7+NVatWabQbGhrCzc0NsbGxsLa2xp07d7BixQqNQyJPa8uWLbC2toaDgwN+/vlnnD59GgsWLAAAjB49Gj/++CPefvttzJo1C1ZWVigqKkJqaipsbGwwbNiwOm/HyMgIAQEBiIqKgrm5OZRKJX755Rfs2bMHP/zwQ4PtT42uXbuiuLgY8fHx6Nu3L06cOIG4uLgG3w4AWFhYQF9fH+vXr8fUqVNx9epVLFu27LF/+d6vX79+UCqVmDt3LhYsWAA9PT0sW7bsge9W7xUbG4v27duje/fuMDQ0xI4dO6CrqwsrKytpjI6ODubNm4fg4GCUlJRg8eLFGDJkCCwtLQHcfb6Sk5Ph7u4OtVqNL7/8EtXV1fV+LuQ0Y8YMBAYGYunSpRgzZgxatmyJ3NxcpKSkYOHChaiursayZcvg7e2Nzp07Q6VS4eDBg9IfTrt378aVK1fQs2dPmJubIyMjA/n5+VJ/165dsW3bNqSmpqJDhw5ISkrCmTNnYGZmJtUwdepUREVFwdraGk5OTti/fz8OHTqk8b1+2jqbCgZNEzR58mT8+9//xrVr1zTalyxZggULFsDPzw/t27fHnDlzcPny5Qbb7ocffojNmzcjLCwM7dq1Q2RkpHSsuOazPitXrpQuz2zdurV0yWZ9zZo1CwqFAkuWLMGNGzfw4osvIjIyEv369Wuw/anh6emJd999FytWrMDt27fRq1cvzJs3Dx9++GGDb8vc3ByRkZFYvnw5EhISYGNjg7CwMEyePLle69HR0cHXX3+NhQsXYsKECWjdujUCAwNRWVn5yOWMjY2xdu1a5ObmQggBa2tr6cWwhpOTE9zd3TF16lSoVCoMHDgQixcvlvqXLl2KRYsWwdfXF23btkVgYCDKy8vrVb/c+vbtix9//BHR0dHw9/eHEAIdO3bEgAED0KJFC+jo6ODWrVv46KOPUFhYCGNjY/Tp0wchISEA7p6nXLduHb755hv8888/6NixI6ZPnw5fX18AQFBQEPLy8hAUFAQ9PT2MHDkSAQEB+Pnnn6Ua3nzzTRQXFyMiIgKVlZUYPHgwpkyZgtWrVzdYnU2Fjnjag75E1GzMnz8f+fn5WLt2bWOX0iyFhobijz/+qPWZqOaO72iIiGTw999/Y/fu3ejTpw8UCgX27duHbdu2SYebnycMGiIiGejq6iIlJQVffvklKioq8OKLL+Jf//oXxo0b19ilaR0PnRERkax4eTMREclKa0Hz1VdfoXv37nB1dZW+Zs+eLfVnZmbCz88Pzs7OGDx4MNatW6exfHl5ORYuXIjevXvDzc0NwcHBGhP9AcD27dvh7e0NJycnjB49GkeOHNHGrhER0SNo9R1Nz549cerUKemrZjbi0tJSTJs2DQMGDMCxY8ewcuVKREdHIyUlRVp2yZIlSE9PR3JyMvbt24fbt29rXOJ38uRJhIWFITQ0FGlpaQgICMD06dNrTYtBRETa9UxcDLBr1y4oFAoEBQVBoVDAxcUFvr6+iIuLwyuvvILy8nIkJSXhq6++kj5lHhISgpEjRyIvLw8WFhbYvHkzhgwZAk9PTwCAr68vNm/ejMTERLz//vv1qufGjX+gVvPUFRFRXSgUOmjduuVD+7UaNOnp6ejbty+MjIykw19dunRBdnY27O3tNebrcnBwkO5Vkpubi4qKCjg6Okr9NjY2MDIyQlZWFiwsLJCdnQ0fHx+N7Tk4OCA7O7vedarVgkFDRNRAtBY0w4cPx9ixY2FhYYGCggJ88cUXmDJlCrZt24bS0lKN2WIBwNTUVJpvqebf+8eYmJhojDE1Na21jrrM/XS/Nm3qNrkgERE9ntaC5t5pvjt06ICIiAjpnI2xsXGtqcJv3bolzSZb869KpZImj6x5fO+Ye++IeP866qOoqJTvaIiI6kih0HnkH+iNdnmzjo6ONO25UqlEZmamxnThGRkZ0v0srKysYGBggPT0dKn/4sWLKCsrk8YolUqN/vvXQUREjUNrQbNz504UFxcDuDt9/YIFC2Bubg5XV1d4e3ujuroaMTExqKysxNmzZxEfH4/x48cDuDsz8ZgxYxAVFYWCggKUlJQgMjISgwYNkm40NG7cOOzduxcHDhxAVVUVEhIScO7cObz22mva2kUiInoArc0M8O677+L06dMoKyuDqampdJ+HmqnHMzMzER4ejqysLGkm2kmTJknLl5eXIyIiAikpKaiuroaHhwfCw8PRqlUracz27dsRFRWF/Px8WFpaIiws7Ilm++WhMyKiunvcoTNOQfMADBoiorp7Zs/REBHR8+GZ+MBmc2NiaghDg9r3qKfnW3lFFVS3nq0bhBFpA4NGBoYGevCft7Gxy6BnTNznE6ACg4aePzx0RkREsmLQEBGRrBg0REQkKwYNERHJikFDRESyYtAQEZGsGDRERCQrBg0REcmKQUNERLJi0BARkawYNEREJCsGDRERyYpBQ0REsmLQEBGRrBg0REQkKwYNERHJikFDRESyYtAQEZGsGDRERCQrBg0REcmKQUNERLJi0BARkawYNEREJCsGDRERyYpBQ0REsmLQEBGRrBg0REQkKwYNERHJikFDRESyYtAQEZGsGi1o3nvvPdjZ2eHo0aNS2+HDhzF69Gg4Oztj+PDh2Llzp8YyN27cQHBwMNzc3NC7d28sXLgQlZWVGmPWrl2LwYMHw9nZGX5+fsjOztbK/hAR0YM1StAkJSWhvLxco+3KlSuYPn06AgICcPz4ccyfPx+hoaE4c+aMNGbOnDm4ffs29u3bh+TkZKSnp+Ozzz6T+nfs2IFVq1Zh5cqVOHbsGAYMGIBp06ahtLRUa/tGRESatB40+fn5WLlyJT755BON9q1bt8LW1ha+vr7Q19eHp6cnPD09sWnTJgB3g+i3335DSEgIzMzM0KFDB8ycOROJiYmoqKgAAGzatAm+vr5wcXGBgYEBgoKCAAC7d+/W7k4SEZFEq0EjhEBYWBimT58OCwsLjb7s7Gw4ODhotDk4OEiHvrKzs2FkZAQbGxup39HREWVlZcjJyXngOhQKBezt7ZGVlSXXLhER0WO00ObG4uLiIITAG2+8UauvtLQUL730kkabqampdNirtLQUJiYmGv01j+8dY2pqWmtMfQ+dtWljXK/xRHXVrp3J4wcRNTNaC5rLly8jJiYGP/300wP7jY2NoVKpNNpu3boFY2Njqf/+wKgZf++Y+9ehUqnQtm3betVaVFQKtVrUa5l78cWEHqawUPX4QURNjEKh88g/0LUWNGlpabh58ybGjh2r0R4UFAQfHx8olUocPHhQoy8jIwNKpRIAoFQqcfv2bVy8eFE6fJaeng5DQ0N07dpVGpOeno5XXnkFAKBWq5GZmYmRI0fKvXtERPQQWjtHM2LECOzevRvbtm2TvgDg008/xezZszFmzBj88ccfSEhIQFVVFQ4cOIB9+/bBz88PANC5c2cMGDAAkZGRKCkpQUFBAaKiojB27FgYGBgAAPz8/BAfH4+zZ8+isrISMTExAIBhw4ZpazeJiOg+WntHY2RkBCMjo1rt5ubmMDMzg5mZGWJiYrB06VKEh4fjv/7rv7BkyRI4OztLYyMjIxEeHg5PT0/o6upixIgRmD9/vtT/6quvorCwEB988AFu3LgBe3t7xMbGSofWiIhI+3SEEE9+MqKZaohzNP7zNjZgRdQcxH0+gedoqFl63DkaTkFDRESyYtAQEZGsGDRERCQrBg0REcmKQUNERLJi0BARkawYNEREJCsGDRERyYpBQ0REsmLQEBGRrBg0REQkKwYNERHJikFDRESyYtAQEZGsGDRERCQrBg0REcmKQUNERLJi0BARkawYNEREJCsGDRERyYpBQ0REsmLQEBGRrBg0REQkKwYNERHJikFDRESyYtAQEZGsGDRERCQrBg0REcmKQUNERLJi0BARkawYNEREJCsGDRERyYpBQ0REstJa0KxatQrDhg2Du7s7+vTpg8DAQGRlZUn9mZmZ8PPzg7OzMwYPHox169ZpLF9eXo6FCxeid+/ecHNzQ3BwMG7evKkxZvv27fD29oaTkxNGjx6NI0eOaGPXiIjoEbQWNCNGjEBCQgJOnDiBgwcP4uWXX8Zbb70FtVqN0tJSTJs2DQMGDMCxY8ewcuVKREdHIyUlRVp+yZIlSE9PR3JyMvbt24fbt28jJCRE6j958iTCwsIQGhqKtLQ0BAQEYPr06cjLy9PWLhIR0QNoLWi6du0KMzOz/9uwQoHCwkKoVCrs2rULCoUCQUFBMDAwgIuLC3x9fREXFwfg7ruZpKQkzJw5Ex06dICZmRlCQkKwf/9+KUg2b96MIUOGwNPTE/r6+vD19UW3bt2QmJiorV0kIqIH0Oo5mv3796Nnz55wdHTEZ599hilTpsDMzAzZ2dmwt7eHQvF/5Tg4OCA7OxsAkJubi4qKCjg6Okr9NjY2MDIykg6/ZWdnw8HBQWN7966DiIgaRwttbmzw4MFIS0vDzZs3kZSUhI4dOwIASktLYWJiojHW1NQUpaWlUj+AWmNMTEw0xpiamtZaR05OTr3rbNPGuN7LENVFu3Ymjx9E1MzUOWjy8vLQsWNH6OjoaLQLIXDt2jVYWFjUeaOtWrXCpEmT0KtXL1hbW8PY2BhFRUUaY27dugVj47sv+DX/qlQqmJubS2NUKpXGGJVK9dB11EdRUSnUalHv5WrwxYQeprBQ9fhBRE2MQqHzyD/Q63zobOjQoSguLq7VfvPmTQwdOrTehanVaty5cweXLl2CUqlEZmYm1Gq11J+RkQGlUgkAsLKygoGBAdLT06X+ixcvoqysTBqjVCo1+u9fBxERNY46B40QD/4Lv7y8HPr6+o9dft26dSgsLAQAFBcXIzw8HPr6+nBxcYG3tzeqq6sRExODyspKnD17FvHx8Rg/fjwAwNDQEGPGjEFUVBQKCgpQUlKCyMhIDBo0CJ06dQIAjBs3Dnv37sWBAwdQVVWFhIQEnDt3Dq+99lpdd5GIiGTw2ENn0dHRAAAdHR18//33eOGFF6Q+tVqNkydPwsbG5rEbSk1NxerVq/HPP//A2NgYjo6OWLt2Ldq2bQsAiI2NRXh4OFavXo3WrVvjvffew4gRI6Tlw8LCEBERgVdffRXV1dXw8PBAeHi41O/m5oaIiAhEREQgPz8flpaWiImJkYKIiIgah4542FuV/8/b2xsAcPnyZXTq1Am6urpSn56eHjp37owPPvig1hVfTVlDnKPxn7exASui5iDu8wk8R0PN0uPO0Tz2Hc2uXbsAAAEBAYiOjtb4LAwREdHj1Pmqs/Xr18tZBxERNVP1+hxNamoqjhw5guvXr2tcIQYAS5cubdDCiIioeahz0Hz77bdYvnw5rK2t0b59+1qfpyEiInqQOgfNxo0bsWDBAkyYMEHOeoiIqJmp8+doVCoVBg4cKGctRETUDNU5aIYNG4bU1FQ5ayEiomaozofOXFxc8OWXX+L8+fNQKpXQ09PT6B81alSDF0dERE1fnYNm8eLFAFDrzpfA3VkDGDRERPQgdQ4a3teFiIiehFZvfEZERM+fOr+jqZlc82Hef//9py6GiIianzoHzc8//6zx+M6dO/j777+hr6+P9u3bM2iIiOiB6hw0NZNr3quoqAghISHw8/Nr0KKIiKj5eKpzNG3atEFwcDAiIyMbqh4iImpmnvpigBYtWqCgoKAhaiEiomaozofOTp48qfFYCIGCggLExsY2q5ueERFRw6pz0Pj7+0NHRwf335DTzc0Nn376aYMXRkREzUOdg2bPnj0ajxUKBczNzWFgYNDgRRERUfNR56Dp1KmTnHUQEVEzVa87bObk5CA2NhYXLlwAAHTr1g2BgYHo2rWrLMUREVHTV+erzg4dOoRRo0YhKysLzs7OcHZ2RmZmJkaPHo0jR47IWSMRETVhdX5Hs3z5cowfPx4fffSRRvunn36KL774Alu2bGnw4oiIqOmr8zuac+fOYfz48bXa/f39ce7cuQYtioiImo86B42xsTHy8/Nrtefl5cHY2LhBiyIiouajzkHj5eWFBQsW4ODBgygrK0NZWRl+/fVXLFq0CF5eXnLWSERETVidz9GEhIQgNDQUb731FnR0dKT24cOHY+7cubIUR0RETV+dg6Zly5aIiorC5cuXNS5v7tKli2zFERFR01fnoJkxYwZ69OiBd955By+++KLU/u233yIzMxMrV66Uoz4iImri6nyOJi0tDQMHDqzVPnDgQKSlpTVoUURE1HzUOWhUKhVeeOGFWu2GhoYoKSlp0KKIiKj5qHPQvPjiizh06FCt9kOHDqFz584NWhQRETUf9bpNwLJly1BZWYn+/fsDuBsyX331FWbNmiVbgURE1LTVOWgmTJiAoqIirFixAv/7v/8LANDX18eUKVMQEBDw2OUjIyOxf/9+XLt2DS+88AIGDhyIuXPnonXr1tKYzMxMLF68GFlZWWjdujWmTp2KSZMmSf3l5eVYsmQJUlJScOfOHQwcOBD/+te/0KpVK2nM9u3bERUVhfz8fFhZWSE0NBT9+vWr624SEVEDq9etnGfMmIHU1FT89NNP+Omnn3DkyBEEBwfXaVldXV1ERkbi6NGj2LZtG/Lz8xEaGir1l5aWYtq0aRgwYACOHTuGlStXIjo6GikpKdKYJUuWID09HcnJydi3bx9u376NkJAQqf/kyZMICwtDaGgo0tLSEBAQgOnTpyMvL68+u0lERA2oXkEDAEZGRnBycoKTk9MDLw54mNmzZ8Pe3h56enpo06YNAgICcOzYMal/165dUCgUCAoKgoGBAVxcXODr64u4uDgAd9/NJCUlYebMmejQoQPMzMwQEhKC/fv3S0GyefNmDBkyBJ6entDX14evry+6deuGxMTE+u4mERE1kHrdj6YhHTlyBEqlUnqcnZ0Ne3t7KBT/l30ODg6Ij48HAOTm5qKiogKOjo5Sv42NDYyMjJCVlQULCwtkZ2fDx8dHYzsODg7Izs6uV21t2nDuNpJHu3YmjV0CkdY1StDs3LkT8fHx2LBhg9RWWloKExPNX0JTU1OUlpZK/QBqjTExMdEYY2pqWmsdOTk59aqvqKgUarWo1zL34osJPUxhoaqxSyBqcAqFziP/QK/3obOntWPHDixatAgxMTHo0aOH1G5sbCwFRo1bt25JM0PX/KtSaf6iqlQqjTH399+7DiIi0j6tBk18fDzCw8PxzTffoG/fvhp9SqUSmZmZUKvVUltGRoZ0eM3KygoGBgZIT0+X+i9evIiysjJpjFKp1Oi/fx1ERKR9WguadevWYdmyZfj+++/h7u5eq9/b2xvV1dWIiYlBZWUlzp49i/j4eOlma4aGhhgzZgyioqJQUFCAkpISREZGYtCgQejUqRMAYNy4cdi7dy8OHDiAqqoqJCQk4Ny5c3jttde0tZtERHQfHSHEk5+MqAc7Ozu0aNEC+vr6Gu07duyAhYUFgLufowkPD5c+RxMYGFjrczQRERFISUlBdXU1PDw8EB4e/tDP0VhaWiIsLKzen6NpiHM0/vM2PvHy1DzFfT6B52ioWXrcORqtBU1TwqAhOTBoqLl65i4GICKi5wuDhoiIZMWgISIiWTFoiIhIVgwaIiKSFYOGiIhkxaAhIiJZMWiIiEhWDBoiIpIVg4aIiGTFoCEiIlkxaIiISFYMGiIikhWDhoiIZMWgISIiWTFoiIhIVgwaIiKSFYOGiIhkxaAhIiJZMWiIiEhWDBoiIpIVg4aIiGTFoCEiIlkxaIiISFYMGiIikhWDhoiIZMWgISIiWTFoiIhIVgwaIiKSFYOGiIhkxaAhIiJZMWiIiEhWDBoiIpKVVoNmx44d8Pf3h5ubG+zs7Gr1Z2Zmws/PD87Ozhg8eDDWrVun0V9eXo6FCxeid+/ecHNzQ3BwMG7evKkxZvv27fD29oaTkxNGjx6NI0eOyLlLRET0GFoNGlNTU/j7+yMsLKxWX2lpKaZNm4YBAwbg2LFjWLlyJaKjo5GSkiKNWbJkCdLT05GcnIx9+/bh9u3bCAkJkfpPnjyJsLAwhIaGIi0tDQEBAZg+fTry8vK0sn9ERFSbVoPGw8MDPj4+6NKlS62+Xbt2QaFQICgoCAYGBnBxcYGvry/i4uIA3H03k5SUhJkzZ6JDhw4wMzNDSEgI9u/fLwXJ5s2bMWTIEHh6ekJfXx++vr7o1q0bEhMTtbmbRER0jxaNXUCN7Oxs2NvbQ6H4v+xzcHBAfHw8ACA3NxcVFRVwdHSU+m1sbGBkZISsrCxYWFggOzsbPj4+Gut1cHBAdnZ2vWpp08b4KfaE6OHatTNp7BKItO6ZCZrS0lKYmGj+EpqamqK0tFTqB1BrjImJicYYU1PTWuvIycmpVy1FRaVQq0W9lrkXX0zoYQoLVY1dAlGDUyh0HvkH+jNz1ZmxsbEUGDVu3boFY2NjqR8AVCrNX1SVSqUx5v7+e9dBRETa98wEjVKpRGZmJtRqtdSWkZEBpVIJALCysoKBgQHS09Ol/osXL6KsrEwao1QqNfrvXwcREWmfVoOmuroaFRUVqKqqAgBUVFSgoqICarUa3t7eqK6uRkxMDCorK3H27FnEx8dj/PjxAABDQ0OMGTMGUVFRKCgoQElJCSIjIzFo0CB06tQJADBu3Djs3bsXBw4cQFVVFRISEnDu3Dm89tpr2txNIiK6h44Q4slPRtRTYmIiQkNDa7WvW7cOffr0QWZmJsLDw5GVlYXWrVsjMDAQkyZNksaVl5cjIiICKSkpqK6uhoeHB8LDw9GqVStpzPbt2xEVFYX8/HxYWloiLCwM/fr1q1edDXGOxn/exidenpqnuM8n8BwNNUuPO0ej1aBpKhg0JAcGDTVXTeZiACIiap4YNEREJCsGDRERyYpBQ0REsnpmZgYgIvm1NtNHC32Dxi6DnjF3Kitwo6RStvUzaIieIy30DXDi82mNXQY9Y9znxQKQL2h46IyIiGTFoCEiIlkxaIiISFYMGiIikhWDhoiIZMWgISIiWTFoiIhIVgwaIiKSFYOGiIhkxaAhIiJZMWiIiEhWDBoiIpIVg4aIiGTFoCEiIlkxaIiISFYMGiIikhWDhoiIZMWgISIiWTFoiIhIVgwaIiKSFYOGiIhkxaAhIiJZMWiIiEhWDBoiIpIVg4aIiGTFoCEiIlkxaIiISFbNLmjUajWWL1+O/v37w9XVFYGBgbh69Wpjl0VE9NxqdkETGxuL7du3Y8OGDfjtt99gYWGBd999F2q1urFLIyJ6LrVo7AIa2qZNmzBt2jRYW1sDAObOnYv+/fvjxIkT6NWrV53WoVDoPHUdbVu3fOp1UPPTED9bT0vftE1jl0DPoKf52XzcsjpCCPHEa3/GqFQq9OzZE/Hx8XBycpLaX331VbzxxhuYNGlSI1ZHRPR8alaHzkpLSwEApqamGu0mJiZSHxERaVezChpjY2MAd9/Z3EulUkl9RESkXc0qaExMTNCpUyekp6dLbSqVCpcvX0b37t0bsTIioudXswoaAPDz88P333+PnJwc3L59G5GRkbCysoK7u3tjl0ZE9FxqdledTZs2DSqVCv7+/igrK4O7uztiYmKgUDS7TCUiahKa1VVnRET07OGf+UREJCsGDRERyYpBQ0REsmLQEBGRrBg0JAvOok3Pqh07dsDf3x9ubm6ws7Nr7HKeCwwakgVn0aZnlampKfz9/REWFtbYpTw3GDQki3tn0W7ZsiXmzp2LnJwcnDhxorFLo+ech4cHfHx80KVLl8Yu5bnBoKEGp1KpcPXqVTg4OEhtpqamsLS0RFZWViNWRkSNgUFDDY6zaBPRvRg01OA4izYR3YtBQw2Os2gT0b0YNCQLzqJNz6rq6mpUVFSgqqoKAFBRUYGKigpeESmjZjd7Mz0bOIs2Pau2bduG0NBQ6XHNbd/XrVuHPn36NFZZzRpnbyYiIlnxz0siIpIVg4aIiGTFoCEiIlkxaIiISFYMGiIikhWDhoiIZMWgIXpGzJ8/H5MnT27sMogaHD+wSaQlN27cwHfffYc9e/YgLy8PxsbGsLa2hq+vL3x8fBq7PCLZMGiItODatWvw9/eHrq4uZsyYAXt7e7Ro0QKnTp3C999/zzs9UrPGoCHSgvDwcFRWViIlJQUmJiZSu5WVFXx8fKR5t+6VkZGBFStWICMjA+Xl5bC2tsbMmTMxcOBAaczu3bsRHR2NnJwc6OnpwcrKCosXL4a9vT2qqqqwbNky/Oc//0FxcTFatWqFXr16YcWKFVrZZ6IaDBoimd28eRMHDhzABx98oBEyNfT09KCnp1ervbS0FCNHjkRISAhatGiBpKQkBAUFITk5GV27dkVhYSGCg4Mxc+ZMvPLKK6isrERmZiZ0dXUBABs2bMB//vMfREZGokuXLrh+/TpOnjwp+/4S3Y9BQySzy5cvQ61W46WXXqrXcvdP8Dhr1izs27cPKSkpmD59OgoLC1FVVYURI0agc+fOAAAbGxtp/NWrV2FlZYXevXtDR0cHFhYW0gSSRNrEoCGS2ZPOW1tcXIyoqCikpqbi+vXr0vT2eXl5AAA7OzsMGDAAo0aNQv/+/dG7d294e3ujY8eOAIDXX38dU6ZMgZeXF/r374+XX34Znp6e0NfXb7B9I6oLXt5MJDNLS0soFApcuHChXsvNnz8fJ06cwNy5c7Fx40YkJSVBqVRK53N0dXURGxuLH3/8EY6Ojti1axeGDx+Offv2AQC6d++OPXv2ICQkBPr6+oiIiMCYMWN4O23SOgYNkcxatWqFgQMHYuPGjbVubw0AVVVVuH37dq3248ePY/z48Rg6dCjs7OzQrl07XLlyRWOMjo4OnJyc8O6772Ljxo3o1asXEhMTpf6WLVvCy8sLH3/8MRISEnDx4kUcO3as4XeS6BEYNERasGjRIrRo0QJjx45FcnIyLly4gEuXLmHbtm14/fXXcenSpVrLdO3aFcnJyfjjjz+QlZWF2bNno7q6Wuo/efIkvv76a5w5cwZ5eXk4cuQI/vjjD+k8TWxsLH7++WecP38ef/31FxISEqCrqwsrKytt7TYRAJ6jIdIKCwsLbN26Fd999x2io6OlD2za2NggMDAQ3bp1q7XM0qVLsWjRIvj6+qJt27YIDAxEeXm51G9iYoLTp08jLi4OJSUlaNeuHUaNGoWgoCAAgLGxMdauXYvc3FwIIWBtbY2oqChYW1trbb+JAN5hk4iIZMZDZ0REJCsGDRERyYpBQ0REsmLQEBGRrBg0REQkKwYNERHJikFDRESyYtAQEZGsGDRERCSr/wcTuA/1Mjf62AAAAABJRU5ErkJggg==\n"},"metadata":{}}]},{"cell_type":"code","source":"df['length'] = df.Text.apply(len)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:01.188427Z","iopub.execute_input":"2024-01-04T13:02:01.188891Z","iopub.status.idle":"2024-01-04T13:02:01.202978Z","shell.execute_reply.started":"2024-01-04T13:02:01.188853Z","shell.execute_reply":"2024-01-04T13:02:01.202331Z"},"trusted":true},"execution_count":112,"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"   Class                                               Text  length\n0      0  Go until jurong point, crazy.. Available only ...     111\n1      0                      Ok lar... Joking wif u oni...      29\n2      1  Free entry in 2 a wkly comp to win FA Cup fina...     155\n3      0  U dun say so early hor... U c already then say...      49\n4      0  Nah I don't think he goes to usf, he lives aro...      61","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Text</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>111</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>155</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>61</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now lets see the SMS length and target relationship","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(df.loc[df.Class == 0, \"length\"], shade=True, label=\"Ham\", clip=(-50, 250),)\nsns.kdeplot(df.loc[df.Class == 1, \"length\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Length\",\n    ylabel=\"Density\",\n    title=\"Length of messages.\",\n)\nax.legend(loc=\"upper right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:01.204008Z","iopub.execute_input":"2024-01-04T13:02:01.204224Z","iopub.status.idle":"2024-01-04T13:02:01.494851Z","shell.execute_reply.started":"2024-01-04T13:02:01.204187Z","shell.execute_reply":"2024-01-04T13:02:01.494093Z"},"trusted":true},"execution_count":113,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"Text preprocessing and cleaning ","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nprint(stop_words[::10])\n\nporter = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:01.496001Z","iopub.execute_input":"2024-01-04T13:02:01.496226Z","iopub.status.idle":"2024-01-04T13:02:01.501841Z","shell.execute_reply.started":"2024-01-04T13:02:01.496196Z","shell.execute_reply":"2024-01-04T13:02:01.501023Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"['i', \"you've\", 'himself', 'they', 'that', 'been', 'a', 'while', 'through', 'in', 'here', 'few', 'own', 'just', 're', 'doesn', 'ma', \"shouldn't\"]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Functions for data cleaning","metadata":{}},{"cell_type":"code","source":"def clean_text(words):\n    \"\"\"The function to clean text\"\"\"\n    words = re.sub(\"[^a-zA-Z]\",\" \", words)\n    text = words.lower().split()                   \n    return \" \".join(text)\n\ndef remove_stopwords(text):\n    \"\"\"The function to removing stopwords\"\"\"\n    text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n    return \" \".join(text)\n\ndef stemmer(stem_text):\n    \"\"\"The function to apply stemming\"\"\"\n    stem_text = [porter.stem(word) for word in stem_text.split()]\n    return \" \".join(stem_text)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:01.505584Z","iopub.execute_input":"2024-01-04T13:02:01.505991Z","iopub.status.idle":"2024-01-04T13:02:01.512766Z","shell.execute_reply.started":"2024-01-04T13:02:01.505961Z","shell.execute_reply":"2024-01-04T13:02:01.511996Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"df['Text'] = df['Text'].apply(clean_text)\ndf['Text'] = df['Text'].apply(remove_stopwords)\ndf['Text'] = df['Text'].apply(stemmer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:01.513682Z","iopub.execute_input":"2024-01-04T13:02:01.513963Z","iopub.status.idle":"2024-01-04T13:02:03.314065Z","shell.execute_reply.started":"2024-01-04T13:02:01.513935Z","shell.execute_reply":"2024-01-04T13:02:03.313406Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.315015Z","iopub.execute_input":"2024-01-04T13:02:03.315228Z","iopub.status.idle":"2024-01-04T13:02:03.324292Z","shell.execute_reply.started":"2024-01-04T13:02:03.315199Z","shell.execute_reply":"2024-01-04T13:02:03.323634Z"},"trusted":true},"execution_count":117,"outputs":[{"execution_count":117,"output_type":"execute_result","data":{"text/plain":"   Class                                               Text  length\n0      0  go jurong point crazi avail bugi n great world...     111\n1      0                              ok lar joke wif u oni      29\n2      1  free entri wkli comp win fa cup final tkt st m...     155\n3      0                u dun say earli hor u c alreadi say      49\n4      0               nah think goe usf live around though      61","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Text</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>go jurong point crazi avail bugi n great world...</td>\n      <td>111</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>ok lar joke wif u oni</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>free entri wkli comp win fa cup final tkt st m...</td>\n      <td>155</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>u dun say earli hor u c alreadi say</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>nah think goe usf live around though</td>\n      <td>61</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X = df['Text']\ny = df['Class']","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.325301Z","iopub.execute_input":"2024-01-04T13:02:03.325511Z","iopub.status.idle":"2024-01-04T13:02:03.334927Z","shell.execute_reply.started":"2024-01-04T13:02:03.325485Z","shell.execute_reply":"2024-01-04T13:02:03.334268Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.335875Z","iopub.execute_input":"2024-01-04T13:02:03.336106Z","iopub.status.idle":"2024-01-04T13:02:03.347402Z","shell.execute_reply.started":"2024-01-04T13:02:03.336066Z","shell.execute_reply":"2024-01-04T13:02:03.346699Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport datasets\nfrom datasets import Dataset, DatasetDict\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(df.rename(columns={\"Text\":\"Text\", \"Class\":\"labels\"}), test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.2)\ntrainh = Dataset.from_pandas(train)\ntesth = Dataset.from_pandas(test)\nvalh = Dataset.from_pandas(val)\n\n\nds = DatasetDict()\n\nds['train'] = trainh\nds['test'] = testh\nds['validation'] = valh\n\nprint(ds)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.348547Z","iopub.execute_input":"2024-01-04T13:02:03.349063Z","iopub.status.idle":"2024-01-04T13:02:03.371550Z","shell.execute_reply.started":"2024-01-04T13:02:03.349024Z","shell.execute_reply":"2024-01-04T13:02:03.370821Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['labels', 'Text', 'length', '__index_level_0__'],\n        num_rows: 3565\n    })\n    test: Dataset({\n        features: ['labels', 'Text', 'length', '__index_level_0__'],\n        num_rows: 1115\n    })\n    validation: Dataset({\n        features: ['labels', 'Text', 'length', '__index_level_0__'],\n        num_rows: 892\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BERT Base","metadata":{}},{"cell_type":"code","source":"# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.372671Z","iopub.execute_input":"2024-01-04T13:02:03.372981Z","iopub.status.idle":"2024-01-04T13:02:03.376173Z","shell.execute_reply.started":"2024-01-04T13:02:03.372940Z","shell.execute_reply":"2024-01-04T13:02:03.375422Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"# bert_model = TFBertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.377327Z","iopub.execute_input":"2024-01-04T13:02:03.377660Z","iopub.status.idle":"2024-01-04T13:02:03.388022Z","shell.execute_reply.started":"2024-01-04T13:02:03.377620Z","shell.execute_reply":"2024-01-04T13:02:03.387381Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"The function which allows to encode our dataset with BERT tokenizer, maximum sentence length is 64 (maxlen).","metadata":{}},{"cell_type":"code","source":"# def encode(text, maxlen):\n#     input_ids=[]\n#     attention_masks=[]\n\n#     for row in text:\n#         encoded = tokenizer.encode_plus(\n#             row,\n#             add_special_tokens=True,\n#             max_length=maxlen,\n#             pad_to_max_length=True,\n#             return_attention_mask=True,\n#         )\n#         input_ids.append(encoded['input_ids'])\n#         attention_masks.append(encoded['attention_mask'])\n\n#     return np.array(input_ids),np.array(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.389100Z","iopub.execute_input":"2024-01-04T13:02:03.389520Z","iopub.status.idle":"2024-01-04T13:02:03.397813Z","shell.execute_reply.started":"2024-01-04T13:02:03.389480Z","shell.execute_reply":"2024-01-04T13:02:03.397169Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"# X_train_input_ids, X_train_attention_masks = encode(X_train.values, maxlen=64)\n# X_test_input_ids, X_test_attention_masks = encode(X_test.values, maxlen=64)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.398649Z","iopub.execute_input":"2024-01-04T13:02:03.398884Z","iopub.status.idle":"2024-01-04T13:02:03.407206Z","shell.execute_reply.started":"2024-01-04T13:02:03.398856Z","shell.execute_reply":"2024-01-04T13:02:03.406599Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"# def build_model(bert_model):\n#     input_word_ids = tf.keras.Input(shape=(64,),dtype='int32')\n#     attention_masks = tf.keras.Input(shape=(64,),dtype='int32')\n\n#     sequence_output = bert_model([input_word_ids,attention_masks])\n#     output = sequence_output[1]\n#     output = tf.keras.layers.Dense(32,activation='relu')(output)\n#     output = tf.keras.layers.Dropout(0.2)(output)\n#     output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n\n#     model = tf.keras.models.Model(inputs = [input_word_ids,attention_masks], outputs = output)\n#     model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n\n#     return model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.408156Z","iopub.execute_input":"2024-01-04T13:02:03.408535Z","iopub.status.idle":"2024-01-04T13:02:03.418430Z","shell.execute_reply.started":"2024-01-04T13:02:03.408498Z","shell.execute_reply":"2024-01-04T13:02:03.417596Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"# model = build_model(bert_model)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.419570Z","iopub.execute_input":"2024-01-04T13:02:03.419942Z","iopub.status.idle":"2024-01-04T13:02:03.427818Z","shell.execute_reply.started":"2024-01-04T13:02:03.419903Z","shell.execute_reply":"2024-01-04T13:02:03.427089Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"# class_weight = {0: 1, 1: 8}","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.428943Z","iopub.execute_input":"2024-01-04T13:02:03.429695Z","iopub.status.idle":"2024-01-04T13:02:03.437340Z","shell.execute_reply.started":"2024-01-04T13:02:03.429656Z","shell.execute_reply":"2024-01-04T13:02:03.436724Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(\n#     [X_train_input_ids, X_train_attention_masks],\n#     y_train,\n#     batch_size=32,\n#     epochs=5,\n#     validation_data=([X_test_input_ids, X_test_attention_masks], y_test),\n#     class_weight=class_weight)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.438285Z","iopub.execute_input":"2024-01-04T13:02:03.438492Z","iopub.status.idle":"2024-01-04T13:02:03.446825Z","shell.execute_reply.started":"2024-01-04T13:02:03.438463Z","shell.execute_reply":"2024-01-04T13:02:03.446196Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"# def plot_graphs(history, string):\n#     plt.plot(history.history[string])\n#     plt.plot(history.history['val_'+string])\n#     plt.xlabel(\"Epochs\")\n#     plt.ylabel(string)\n#     plt.legend([string, 'val_'+string])\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.447781Z","iopub.execute_input":"2024-01-04T13:02:03.447964Z","iopub.status.idle":"2024-01-04T13:02:03.456560Z","shell.execute_reply.started":"2024-01-04T13:02:03.447941Z","shell.execute_reply":"2024-01-04T13:02:03.455878Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"# loss, accuracy = model.evaluate([X_test_input_ids, X_test_attention_masks], y_test)\n# print('Test accuracy :', accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.457546Z","iopub.execute_input":"2024-01-04T13:02:03.457792Z","iopub.status.idle":"2024-01-04T13:02:03.469923Z","shell.execute_reply.started":"2024-01-04T13:02:03.457755Z","shell.execute_reply":"2024-01-04T13:02:03.469237Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorWithPadding\n\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ndef tokenize_function(example):\n    return tokenizer(example[\"Text\"], truncation=True)\n\n\ntokenized_datasets = ds.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:03.470865Z","iopub.execute_input":"2024-01-04T13:02:03.471505Z","iopub.status.idle":"2024-01-04T13:02:05.055761Z","shell.execute_reply.started":"2024-01-04T13:02:03.471463Z","shell.execute_reply":"2024-01-04T13:02:05.055061Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\nloading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\nloading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd57f5a548484e798e16e56426adb673"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd0961bd1b94665ab512b767cfd3a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fbd6234dde34f0abd1c745257e32174"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntraining_args = TrainingArguments(\"test-trainer\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:05.056842Z","iopub.execute_input":"2024-01-04T13:02:05.057056Z","iopub.status.idle":"2024-01-04T13:02:05.065941Z","shell.execute_reply.started":"2024-01-04T13:02:05.057028Z","shell.execute_reply":"2024-01-04T13:02:05.065198Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:05.067088Z","iopub.execute_input":"2024-01-04T13:02:05.067295Z","iopub.status.idle":"2024-01-04T13:02:06.476351Z","shell.execute_reply.started":"2024-01-04T13:02:05.067269Z","shell.execute_reply":"2024-01-04T13:02:06.475627Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:06.482311Z","iopub.execute_input":"2024-01-04T13:02:06.482564Z","iopub.status.idle":"2024-01-04T13:02:06.491360Z","shell.execute_reply.started":"2024-01-04T13:02:06.482536Z","shell.execute_reply":"2024-01-04T13:02:06.490633Z"},"trusted":true},"execution_count":134,"outputs":[{"execution_count":134,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:06.492498Z","iopub.execute_input":"2024-01-04T13:02:06.492782Z","iopub.status.idle":"2024-01-04T13:02:06.618192Z","shell.execute_reply.started":"2024-01-04T13:02:06.492722Z","shell.execute_reply":"2024-01-04T13:02:06.617510Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:02:06.619766Z","iopub.execute_input":"2024-01-04T13:02:06.620003Z","iopub.status.idle":"2024-01-04T13:03:41.857981Z","shell.execute_reply.started":"2024-01-04T13:02:06.619974Z","shell.execute_reply":"2024-01-04T13:03:41.857188Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stderr","text":"The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 3565\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1338' max='1338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1338/1338 01:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.146200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.041200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to test-trainer/checkpoint-500\nConfiguration saved in test-trainer/checkpoint-500/config.json\nModel weights saved in test-trainer/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-1000\nConfiguration saved in test-trainer/checkpoint-1000/config.json\nModel weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":136,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1338, training_loss=0.0737930612891957, metrics={'train_runtime': 95.2148, 'train_samples_per_second': 112.325, 'train_steps_per_second': 14.052, 'total_flos': 161677118519700.0, 'train_loss': 0.0737930612891957, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_datasets[\"test\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:41.859042Z","iopub.execute_input":"2024-01-04T13:03:41.859262Z","iopub.status.idle":"2024-01-04T13:03:43.822044Z","shell.execute_reply.started":"2024-01-04T13:03:41.859234Z","shell.execute_reply":"2024-01-04T13:03:43.821180Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 1115\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"(1115, 2) (1115,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\npreds = np.argmax(predictions.predictions, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:43.823573Z","iopub.execute_input":"2024-01-04T13:03:43.823922Z","iopub.status.idle":"2024-01-04T13:03:43.829349Z","shell.execute_reply.started":"2024-01-04T13:03:43.823876Z","shell.execute_reply":"2024-01-04T13:03:43.828431Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nx = accuracy_score(preds, y_test)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:43.830934Z","iopub.execute_input":"2024-01-04T13:03:43.831189Z","iopub.status.idle":"2024-01-04T13:03:43.843009Z","shell.execute_reply.started":"2024-01-04T13:03:43.831157Z","shell.execute_reply":"2024-01-04T13:03:43.842175Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"0.7650224215246637\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BERT Large","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorWithPadding\n\ncheckpoint = \"bert-large-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ndef tokenize_function(example):\n    return tokenizer(example[\"Text\"], truncation=True)\n\n\ntokenized_datasets = ds.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:43.843997Z","iopub.execute_input":"2024-01-04T13:03:43.844195Z","iopub.status.idle":"2024-01-04T13:03:45.796193Z","shell.execute_reply.started":"2024-01-04T13:03:43.844170Z","shell.execute_reply":"2024-01-04T13:03:45.795464Z"},"trusted":true},"execution_count":140,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\nModel config BertConfig {\n  \"_name_or_path\": \"bert-large-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading file https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e12f02d630da91a0982ce6db1ad595231d155a2b725ab106971898276d842ecc.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/bert-large-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/475d46024228961ca8770cead39e1079f135fd2441d14cf216727ffac8d41d78.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\nloading file https://huggingface.co/bert-large-uncased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/bert-large-uncased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/bert-large-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/300ecd79785b4602752c0085f8a89c3f0232ef367eda291c79a5600f3778b677.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\nloading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\nModel config BertConfig {\n  \"_name_or_path\": \"bert-large-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6436d6e3344c4957aa09389578255727"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"760e9f037878470c9dd3b58a01a0e192"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed23260bf1a4ed99450cac1d9ad1758"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntraining_args = TrainingArguments(\"test-trainer\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:45.797488Z","iopub.execute_input":"2024-01-04T13:03:45.797872Z","iopub.status.idle":"2024-01-04T13:03:45.806844Z","shell.execute_reply.started":"2024-01-04T13:03:45.797828Z","shell.execute_reply":"2024-01-04T13:03:45.806060Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:45.808099Z","iopub.execute_input":"2024-01-04T13:03:45.808332Z","iopub.status.idle":"2024-01-04T13:03:50.472383Z","shell.execute_reply.started":"2024-01-04T13:03:45.808304Z","shell.execute_reply":"2024-01-04T13:03:50.471657Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\nModel config BertConfig {\n  \"_name_or_path\": \"bert-large-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/bert-large-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1d959166dd7e047e57ea1b2d9b7b9669938a7e90c5e37a03961ad9f15eaea17f.fea64cd906e3766b04c92397f9ad3ff45271749cbe49829a079dd84e34c1697d\nSome weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:50.473337Z","iopub.execute_input":"2024-01-04T13:03:50.473535Z","iopub.status.idle":"2024-01-04T13:03:50.485689Z","shell.execute_reply.started":"2024-01-04T13:03:50.473509Z","shell.execute_reply":"2024-01-04T13:03:50.484983Z"},"trusted":true},"execution_count":143,"outputs":[{"execution_count":143,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n      (position_embeddings): Embedding(512, 1024)\n      (token_type_embeddings): Embedding(2, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (12): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (13): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (14): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (15): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (16): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (17): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (18): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (19): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (20): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (21): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (22): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (23): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:50.486767Z","iopub.execute_input":"2024-01-04T13:03:50.486992Z","iopub.status.idle":"2024-01-04T13:03:50.825487Z","shell.execute_reply.started":"2024-01-04T13:03:50.486965Z","shell.execute_reply":"2024-01-04T13:03:50.824790Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:03:50.826856Z","iopub.execute_input":"2024-01-04T13:03:50.827123Z","iopub.status.idle":"2024-01-04T13:08:41.454774Z","shell.execute_reply.started":"2024-01-04T13:03:50.827088Z","shell.execute_reply":"2024-01-04T13:08:41.454005Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stderr","text":"The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 3565\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1338' max='1338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1338/1338 04:50, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.321400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.404800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to test-trainer/checkpoint-500\nConfiguration saved in test-trainer/checkpoint-500/config.json\nModel weights saved in test-trainer/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-1000\nConfiguration saved in test-trainer/checkpoint-1000/config.json\nModel weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1338, training_loss=0.3774566650390625, metrics={'train_runtime': 290.6004, 'train_samples_per_second': 36.803, 'train_steps_per_second': 4.604, 'total_flos': 572655441159060.0, 'train_loss': 0.3774566650390625, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_datasets[\"test\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:41.455929Z","iopub.execute_input":"2024-01-04T13:08:41.456153Z","iopub.status.idle":"2024-01-04T13:08:46.484543Z","shell.execute_reply.started":"2024-01-04T13:08:41.456124Z","shell.execute_reply":"2024-01-04T13:08:46.483806Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 1115\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"(1115, 2) (1115,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\npreds = np.argmax(predictions.predictions, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:46.485677Z","iopub.execute_input":"2024-01-04T13:08:46.485969Z","iopub.status.idle":"2024-01-04T13:08:46.490344Z","shell.execute_reply.started":"2024-01-04T13:08:46.485929Z","shell.execute_reply":"2024-01-04T13:08:46.489586Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nx = accuracy_score(preds, y_test)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:46.491582Z","iopub.execute_input":"2024-01-04T13:08:46.492113Z","iopub.status.idle":"2024-01-04T13:08:46.510211Z","shell.execute_reply.started":"2024-01-04T13:08:46.492072Z","shell.execute_reply":"2024-01-04T13:08:46.509455Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"0.8511210762331839\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BERT Medium","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorWithPadding\n\ncheckpoint = \"bert-large-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ndef tokenize_function(example):\n    return tokenizer(example[\"Text\"], truncation=True)\n\n\ntokenized_datasets = ds.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:46.511405Z","iopub.execute_input":"2024-01-04T13:08:46.511884Z","iopub.status.idle":"2024-01-04T13:08:48.327998Z","shell.execute_reply.started":"2024-01-04T13:08:46.511843Z","shell.execute_reply":"2024-01-04T13:08:48.327224Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\nModel config BertConfig {\n  \"_name_or_path\": \"bert-large-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading file https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/e12f02d630da91a0982ce6db1ad595231d155a2b725ab106971898276d842ecc.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/bert-large-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/475d46024228961ca8770cead39e1079f135fd2441d14cf216727ffac8d41d78.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\nloading file https://huggingface.co/bert-large-uncased/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/bert-large-uncased/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/bert-large-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/300ecd79785b4602752c0085f8a89c3f0232ef367eda291c79a5600f3778b677.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\nloading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\nModel config BertConfig {\n  \"_name_or_path\": \"bert-large-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9b9b664f88642cb95b7514469bb8289"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8867ce9e623e4d478bf9f90a54c81fc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c15c848b8914530a0747268a79af81c"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntraining_args = TrainingArguments(\"test-trainer\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:48.329044Z","iopub.execute_input":"2024-01-04T13:08:48.329270Z","iopub.status.idle":"2024-01-04T13:08:48.337933Z","shell.execute_reply.started":"2024-01-04T13:08:48.329241Z","shell.execute_reply":"2024-01-04T13:08:48.337228Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-medium\", num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:48.338971Z","iopub.execute_input":"2024-01-04T13:08:48.339210Z","iopub.status.idle":"2024-01-04T13:08:49.051119Z","shell.execute_reply.started":"2024-01-04T13:08:48.339173Z","shell.execute_reply":"2024-01-04T13:08:49.050358Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/288b0ee1e79a7c3fe770ab8a84ece013c573e7d226ccb5d9ffad317b3419faac.4344f82f77799c092b30b2e0d3749c809f82df14c5993e43dbbdc52f5a0d86e0\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-medium\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 8,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dabb6f3bc29449f038f41cb09eb1a693eee2bee3dab8afff878a2910fa73a171.b722b1c13187b9ed20e5e36ab761041218e88d502895424e3ed2516bc9693089\nSome weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-medium and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:49.052229Z","iopub.execute_input":"2024-01-04T13:08:49.052453Z","iopub.status.idle":"2024-01-04T13:08:49.060334Z","shell.execute_reply.started":"2024-01-04T13:08:49.052425Z","shell.execute_reply":"2024-01-04T13:08:49.059530Z"},"trusted":true},"execution_count":152,"outputs":[{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n      (position_embeddings): Embedding(512, 512)\n      (token_type_embeddings): Embedding(2, 512)\n      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=512, out_features=512, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=512, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:49.061304Z","iopub.execute_input":"2024-01-04T13:08:49.061524Z","iopub.status.idle":"2024-01-04T13:08:49.128955Z","shell.execute_reply.started":"2024-01-04T13:08:49.061487Z","shell.execute_reply":"2024-01-04T13:08:49.128050Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:08:49.130517Z","iopub.execute_input":"2024-01-04T13:08:49.130798Z","iopub.status.idle":"2024-01-04T13:09:49.796360Z","shell.execute_reply.started":"2024-01-04T13:08:49.130756Z","shell.execute_reply":"2024-01-04T13:09:49.795598Z"},"trusted":true},"execution_count":154,"outputs":[{"name":"stderr","text":"The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 3565\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1338' max='1338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1338/1338 01:00, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.127000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.044000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to test-trainer/checkpoint-500\nConfiguration saved in test-trainer/checkpoint-500/config.json\nModel weights saved in test-trainer/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-1000\nConfiguration saved in test-trainer/checkpoint-1000/config.json\nModel weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":154,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1338, training_loss=0.0697315911719261, metrics={'train_runtime': 60.643, 'train_samples_per_second': 176.36, 'train_steps_per_second': 22.064, 'total_flos': 48105472892820.0, 'train_loss': 0.0697315911719261, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_datasets[\"test\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:49.797674Z","iopub.execute_input":"2024-01-04T13:09:49.798061Z","iopub.status.idle":"2024-01-04T13:09:51.150284Z","shell.execute_reply.started":"2024-01-04T13:09:49.798022Z","shell.execute_reply":"2024-01-04T13:09:51.149493Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 1115\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"(1115, 2) (1115,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\npreds = np.argmax(predictions.predictions, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:51.151325Z","iopub.execute_input":"2024-01-04T13:09:51.151543Z","iopub.status.idle":"2024-01-04T13:09:51.155706Z","shell.execute_reply.started":"2024-01-04T13:09:51.151515Z","shell.execute_reply":"2024-01-04T13:09:51.154984Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nx = accuracy_score(preds, y_test)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:51.156977Z","iopub.execute_input":"2024-01-04T13:09:51.157237Z","iopub.status.idle":"2024-01-04T13:09:51.172779Z","shell.execute_reply.started":"2024-01-04T13:09:51.157201Z","shell.execute_reply":"2024-01-04T13:09:51.172007Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"0.7650224215246637\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BERT Small","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorWithPadding\n\ncheckpoint = \"prajjwal1/bert-small\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ndef tokenize_function(example):\n    return tokenizer(example[\"Text\"], truncation=True)\n\n\ntokenized_datasets = ds.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:51.173991Z","iopub.execute_input":"2024-01-04T13:09:51.174212Z","iopub.status.idle":"2024-01-04T13:09:53.053508Z","shell.execute_reply.started":"2024-01-04T13:09:51.174185Z","shell.execute_reply":"2024-01-04T13:09:53.052762Z"},"trusted":true},"execution_count":158,"outputs":[{"name":"stderr","text":"Could not locate the tokenizer configuration file, will try to use the model config instead.\nloading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-small\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 4,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading file https://huggingface.co/prajjwal1/bert-small/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/68be80309844e53b628e9d479926a991d0adf337752bb941f0188887240313b8.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-small/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-small/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer_config.json from cache at None\nloading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-small\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 4,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-small\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 4,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00aeaf427b18475c866ccd9c9337a1b2"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c65cded92bac469ea4384beab44d48ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8afd9ba3a145452498cbe994f2852df2"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntraining_args = TrainingArguments(\"test-trainer\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:53.054595Z","iopub.execute_input":"2024-01-04T13:09:53.054811Z","iopub.status.idle":"2024-01-04T13:09:53.063411Z","shell.execute_reply.started":"2024-01-04T13:09:53.054783Z","shell.execute_reply":"2024-01-04T13:09:53.062715Z"},"trusted":true},"execution_count":159,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:53.064636Z","iopub.execute_input":"2024-01-04T13:09:53.064876Z","iopub.status.idle":"2024-01-04T13:09:53.628211Z","shell.execute_reply.started":"2024-01-04T13:09:53.064847Z","shell.execute_reply":"2024-01-04T13:09:53.627495Z"},"trusted":true},"execution_count":160,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-small\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 512,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 2048,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 8,\n  \"num_hidden_layers\": 4,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/facfdb1638fdec899406e0efd5c2c43ae4bbafcb45dd15f68df1f2378e3e70fb.59547972ec02ba39d4ea413c843f1638e8f90e118a4334ae5d626bf7524ac597\nSome weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:53.629308Z","iopub.execute_input":"2024-01-04T13:09:53.629535Z","iopub.status.idle":"2024-01-04T13:09:53.636957Z","shell.execute_reply.started":"2024-01-04T13:09:53.629505Z","shell.execute_reply":"2024-01-04T13:09:53.636214Z"},"trusted":true},"execution_count":161,"outputs":[{"execution_count":161,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n      (position_embeddings): Embedding(512, 512)\n      (token_type_embeddings): Embedding(2, 512)\n      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=512, out_features=512, bias=True)\n              (key): Linear(in_features=512, out_features=512, bias=True)\n              (value): Linear(in_features=512, out_features=512, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=512, out_features=512, bias=True)\n              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=512, out_features=2048, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=2048, out_features=512, bias=True)\n            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=512, out_features=512, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=512, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:53.638052Z","iopub.execute_input":"2024-01-04T13:09:53.638286Z","iopub.status.idle":"2024-01-04T13:09:53.688642Z","shell.execute_reply.started":"2024-01-04T13:09:53.638245Z","shell.execute_reply":"2024-01-04T13:09:53.687797Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:09:53.689801Z","iopub.execute_input":"2024-01-04T13:09:53.690038Z","iopub.status.idle":"2024-01-04T13:10:28.453382Z","shell.execute_reply.started":"2024-01-04T13:09:53.690008Z","shell.execute_reply":"2024-01-04T13:10:28.452543Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stderr","text":"The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 3565\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1338' max='1338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1338/1338 00:34, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.129200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.048600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to test-trainer/checkpoint-500\nConfiguration saved in test-trainer/checkpoint-500/config.json\nModel weights saved in test-trainer/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-1000\nConfiguration saved in test-trainer/checkpoint-1000/config.json\nModel weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":163,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1338, training_loss=0.07344105781817471, metrics={'train_runtime': 34.7419, 'train_samples_per_second': 307.841, 'train_steps_per_second': 38.513, 'total_flos': 24302577880980.0, 'train_loss': 0.07344105781817471, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_datasets[\"test\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:28.454564Z","iopub.execute_input":"2024-01-04T13:10:28.454829Z","iopub.status.idle":"2024-01-04T13:10:29.429880Z","shell.execute_reply.started":"2024-01-04T13:10:28.454799Z","shell.execute_reply":"2024-01-04T13:10:29.429169Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 1115\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"(1115, 2) (1115,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\npreds = np.argmax(predictions.predictions, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:29.430845Z","iopub.execute_input":"2024-01-04T13:10:29.431030Z","iopub.status.idle":"2024-01-04T13:10:29.435421Z","shell.execute_reply.started":"2024-01-04T13:10:29.431004Z","shell.execute_reply":"2024-01-04T13:10:29.434659Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nx = accuracy_score(preds, y_test)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:29.436554Z","iopub.execute_input":"2024-01-04T13:10:29.436829Z","iopub.status.idle":"2024-01-04T13:10:29.447279Z","shell.execute_reply.started":"2024-01-04T13:10:29.436792Z","shell.execute_reply":"2024-01-04T13:10:29.446508Z"},"trusted":true},"execution_count":166,"outputs":[{"name":"stdout","text":"0.7596412556053812\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BERT Tiny","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorWithPadding\n\ncheckpoint = \"prajjwal1/bert-tiny\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ndef tokenize_function(example):\n    return tokenizer(example[\"Text\"], truncation=True)\n\n\ntokenized_datasets = ds.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:29.448357Z","iopub.execute_input":"2024-01-04T13:10:29.448572Z","iopub.status.idle":"2024-01-04T13:10:31.305172Z","shell.execute_reply.started":"2024-01-04T13:10:29.448543Z","shell.execute_reply":"2024-01-04T13:10:31.304414Z"},"trusted":true},"execution_count":167,"outputs":[{"name":"stderr","text":"Could not locate the tokenizer configuration file, will try to use the model config instead.\nloading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\nloading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dcbd5e754314d5e9069b86bb2f2e68f"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86babcd765614b0795ee9ccdcb57c7b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27cd5215e4044268918dfab1765e5db"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntraining_args = TrainingArguments(\"test-trainer\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:31.306353Z","iopub.execute_input":"2024-01-04T13:10:31.306629Z","iopub.status.idle":"2024-01-04T13:10:31.316178Z","shell.execute_reply.started":"2024-01-04T13:10:31.306590Z","shell.execute_reply":"2024-01-04T13:10:31.315324Z"},"trusted":true},"execution_count":168,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:31.317328Z","iopub.execute_input":"2024-01-04T13:10:31.317554Z","iopub.status.idle":"2024-01-04T13:10:31.731235Z","shell.execute_reply.started":"2024-01-04T13:10:31.317521Z","shell.execute_reply":"2024-01-04T13:10:31.730330Z"},"trusted":true},"execution_count":169,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 128,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 512,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 2,\n  \"num_hidden_layers\": 2,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\nSome weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:31.732592Z","iopub.execute_input":"2024-01-04T13:10:31.733234Z","iopub.status.idle":"2024-01-04T13:10:31.740390Z","shell.execute_reply.started":"2024-01-04T13:10:31.733179Z","shell.execute_reply":"2024-01-04T13:10:31.739562Z"},"trusted":true},"execution_count":170,"outputs":[{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=128, out_features=128, bias=True)\n              (key): Linear(in_features=128, out_features=128, bias=True)\n              (value): Linear(in_features=128, out_features=128, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=128, out_features=128, bias=True)\n              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=128, out_features=512, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=512, out_features=128, bias=True)\n            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=128, out_features=128, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=128, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:31.741864Z","iopub.execute_input":"2024-01-04T13:10:31.742120Z","iopub.status.idle":"2024-01-04T13:10:31.765527Z","shell.execute_reply.started":"2024-01-04T13:10:31.742085Z","shell.execute_reply":"2024-01-04T13:10:31.764786Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:31.767014Z","iopub.execute_input":"2024-01-04T13:10:31.767255Z","iopub.status.idle":"2024-01-04T13:10:52.182270Z","shell.execute_reply.started":"2024-01-04T13:10:31.767226Z","shell.execute_reply":"2024-01-04T13:10:52.181524Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stderr","text":"The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 3565\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1338' max='1338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1338/1338 00:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.216800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.084500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to test-trainer/checkpoint-500\nConfiguration saved in test-trainer/checkpoint-500/config.json\nModel weights saved in test-trainer/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-1000\nConfiguration saved in test-trainer/checkpoint-1000/config.json\nModel weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1338, training_loss=0.13082313822702085, metrics={'train_runtime': 20.3945, 'train_samples_per_second': 524.407, 'train_steps_per_second': 65.606, 'total_flos': 780691953300.0, 'train_loss': 0.13082313822702085, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_datasets[\"test\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:52.183315Z","iopub.execute_input":"2024-01-04T13:10:52.183529Z","iopub.status.idle":"2024-01-04T13:10:52.980211Z","shell.execute_reply.started":"2024-01-04T13:10:52.183501Z","shell.execute_reply":"2024-01-04T13:10:52.979458Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 1115\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"(1115, 2) (1115,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\npreds = np.argmax(predictions.predictions, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:52.981398Z","iopub.execute_input":"2024-01-04T13:10:52.981935Z","iopub.status.idle":"2024-01-04T13:10:52.986390Z","shell.execute_reply.started":"2024-01-04T13:10:52.981893Z","shell.execute_reply":"2024-01-04T13:10:52.985582Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nx = accuracy_score(preds, y_test)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:52.987363Z","iopub.execute_input":"2024-01-04T13:10:52.987570Z","iopub.status.idle":"2024-01-04T13:10:52.998719Z","shell.execute_reply.started":"2024-01-04T13:10:52.987543Z","shell.execute_reply":"2024-01-04T13:10:52.997918Z"},"trusted":true},"execution_count":175,"outputs":[{"name":"stdout","text":"0.7650224215246637\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## BERT Mini","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, DataCollatorWithPadding\n\ncheckpoint = \"prajjwal1/bert-mini\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ndef tokenize_function(example):\n    return tokenizer(example[\"Text\"], truncation=True)\n\n\ntokenized_datasets = ds.map(tokenize_function, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:52.999699Z","iopub.execute_input":"2024-01-04T13:10:52.999959Z","iopub.status.idle":"2024-01-04T13:10:55.136173Z","shell.execute_reply.started":"2024-01-04T13:10:52.999930Z","shell.execute_reply":"2024-01-04T13:10:55.135444Z"},"trusted":true},"execution_count":176,"outputs":[{"name":"stderr","text":"Could not locate the tokenizer configuration file, will try to use the model config instead.\nhttps://huggingface.co/prajjwal1/bert-mini/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnt4x0b1u\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/286 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758c220570374b659abdb7d9dd15fe49"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/prajjwal1/bert-mini/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a32529b12a03c02e99c269bf68c0c7b8349093f626e860ab9b012e3d9539c539.e6c2a1d71adb3143ecd42222c4604e92ff255a7663c04bb5c4fad770c78e096c\ncreating metadata file for /root/.cache/huggingface/transformers/a32529b12a03c02e99c269bf68c0c7b8349093f626e860ab9b012e3d9539c539.e6c2a1d71adb3143ecd42222c4604e92ff255a7663c04bb5c4fad770c78e096c\nloading configuration file https://huggingface.co/prajjwal1/bert-mini/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a32529b12a03c02e99c269bf68c0c7b8349093f626e860ab9b012e3d9539c539.e6c2a1d71adb3143ecd42222c4604e92ff255a7663c04bb5c4fad770c78e096c\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-mini\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 256,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 1024,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 4,\n  \"num_hidden_layers\": 4,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nhttps://huggingface.co/prajjwal1/bert-mini/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj3lnaf91\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f8fad467564ec494e48e91e540f5c0"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/prajjwal1/bert-mini/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/62f8357e13eddc9798915fddaeb0de8bb9a14deda654be17fbfd049a56dd3b5a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\ncreating metadata file for /root/.cache/huggingface/transformers/62f8357e13eddc9798915fddaeb0de8bb9a14deda654be17fbfd049a56dd3b5a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/prajjwal1/bert-mini/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/62f8357e13eddc9798915fddaeb0de8bb9a14deda654be17fbfd049a56dd3b5a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\nloading file https://huggingface.co/prajjwal1/bert-mini/resolve/main/tokenizer.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-mini/resolve/main/added_tokens.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-mini/resolve/main/special_tokens_map.json from cache at None\nloading file https://huggingface.co/prajjwal1/bert-mini/resolve/main/tokenizer_config.json from cache at None\nloading configuration file https://huggingface.co/prajjwal1/bert-mini/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a32529b12a03c02e99c269bf68c0c7b8349093f626e860ab9b012e3d9539c539.e6c2a1d71adb3143ecd42222c4604e92ff255a7663c04bb5c4fad770c78e096c\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-mini\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 256,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 1024,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 4,\n  \"num_hidden_layers\": 4,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading configuration file https://huggingface.co/prajjwal1/bert-mini/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a32529b12a03c02e99c269bf68c0c7b8349093f626e860ab9b012e3d9539c539.e6c2a1d71adb3143ecd42222c4604e92ff255a7663c04bb5c4fad770c78e096c\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-mini\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 256,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 1024,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 4,\n  \"num_hidden_layers\": 4,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94d7f894376d4e7e829e21e804601536"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5fd4019db841f5818681b82b3a6f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f6bdc8da0704545a834a24420840c1b"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntraining_args = TrainingArguments(\"test-trainer\")","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:55.137224Z","iopub.execute_input":"2024-01-04T13:10:55.137446Z","iopub.status.idle":"2024-01-04T13:10:55.146066Z","shell.execute_reply.started":"2024-01-04T13:10:55.137418Z","shell.execute_reply":"2024-01-04T13:10:55.145449Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:55.147332Z","iopub.execute_input":"2024-01-04T13:10:55.147589Z","iopub.status.idle":"2024-01-04T13:10:56.769259Z","shell.execute_reply.started":"2024-01-04T13:10:55.147545Z","shell.execute_reply":"2024-01-04T13:10:56.768516Z"},"trusted":true},"execution_count":178,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/prajjwal1/bert-mini/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a32529b12a03c02e99c269bf68c0c7b8349093f626e860ab9b012e3d9539c539.e6c2a1d71adb3143ecd42222c4604e92ff255a7663c04bb5c4fad770c78e096c\nModel config BertConfig {\n  \"_name_or_path\": \"prajjwal1/bert-mini\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 256,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 1024,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 4,\n  \"num_hidden_layers\": 4,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.18.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nhttps://huggingface.co/prajjwal1/bert-mini/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplnucnl4c\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/43.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5090a0d5b54e4f3eb072ff324d353838"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/prajjwal1/bert-mini/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/3baee60ec6103a88d346bbdcc74e81e9027137f2d2a589e1031cb569ce2c1101.0eab9dd6f6881374d284b4961e8bd581e67d6829624515d67c7cd26d65d8aaaf\ncreating metadata file for /root/.cache/huggingface/transformers/3baee60ec6103a88d346bbdcc74e81e9027137f2d2a589e1031cb569ce2c1101.0eab9dd6f6881374d284b4961e8bd581e67d6829624515d67c7cd26d65d8aaaf\nloading weights file https://huggingface.co/prajjwal1/bert-mini/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/3baee60ec6103a88d346bbdcc74e81e9027137f2d2a589e1031cb569ce2c1101.0eab9dd6f6881374d284b4961e8bd581e67d6829624515d67c7cd26d65d8aaaf\nSome weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:56.770534Z","iopub.execute_input":"2024-01-04T13:10:56.770912Z","iopub.status.idle":"2024-01-04T13:10:56.777847Z","shell.execute_reply.started":"2024-01-04T13:10:56.770869Z","shell.execute_reply":"2024-01-04T13:10:56.777104Z"},"trusted":true},"execution_count":179,"outputs":[{"execution_count":179,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n      (position_embeddings): Embedding(512, 256)\n      (token_type_embeddings): Embedding(2, 256)\n      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=256, out_features=256, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=256, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:56.779108Z","iopub.execute_input":"2024-01-04T13:10:56.779446Z","iopub.status.idle":"2024-01-04T13:10:56.812662Z","shell.execute_reply.started":"2024-01-04T13:10:56.779409Z","shell.execute_reply":"2024-01-04T13:10:56.811984Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:10:56.814279Z","iopub.execute_input":"2024-01-04T13:10:56.814613Z","iopub.status.idle":"2024-01-04T13:11:29.759087Z","shell.execute_reply.started":"2024-01-04T13:10:56.814574Z","shell.execute_reply":"2024-01-04T13:11:29.758366Z"},"trusted":true},"execution_count":181,"outputs":[{"name":"stderr","text":"The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 3565\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1338' max='1338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1338/1338 00:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.141800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.067600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to test-trainer/checkpoint-500\nConfiguration saved in test-trainer/checkpoint-500/config.json\nModel weights saved in test-trainer/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\nSaving model checkpoint to test-trainer/checkpoint-1000\nConfiguration saved in test-trainer/checkpoint-1000/config.json\nModel weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":181,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1338, training_loss=0.0924873195243345, metrics={'train_runtime': 32.9228, 'train_samples_per_second': 324.851, 'train_steps_per_second': 40.641, 'total_flos': 6089419888020.0, 'train_loss': 0.0924873195243345, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_datasets[\"test\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:11:29.760221Z","iopub.execute_input":"2024-01-04T13:11:29.760445Z","iopub.status.idle":"2024-01-04T13:11:30.763420Z","shell.execute_reply.started":"2024-01-04T13:11:29.760417Z","shell.execute_reply":"2024-01-04T13:11:30.762548Z"},"trusted":true},"execution_count":182,"outputs":[{"name":"stderr","text":"The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: Text, __index_level_0__, length. If Text, __index_level_0__, length are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Prediction *****\n  Num examples = 1115\n  Batch size = 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"(1115, 2) (1115,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\npreds = np.argmax(predictions.predictions, axis=-1)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:11:30.764627Z","iopub.execute_input":"2024-01-04T13:11:30.764877Z","iopub.status.idle":"2024-01-04T13:11:30.769105Z","shell.execute_reply.started":"2024-01-04T13:11:30.764847Z","shell.execute_reply":"2024-01-04T13:11:30.768209Z"},"trusted":true},"execution_count":183,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nx = accuracy_score(preds, y_test)\nprint(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T13:11:30.770356Z","iopub.execute_input":"2024-01-04T13:11:30.770799Z","iopub.status.idle":"2024-01-04T13:11:30.783527Z","shell.execute_reply.started":"2024-01-04T13:11:30.770726Z","shell.execute_reply":"2024-01-04T13:11:30.782795Z"},"trusted":true},"execution_count":184,"outputs":[{"name":"stdout","text":"0.7659192825112108\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}