{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/izzymohamed/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure you have the VADER lexicon downloaded\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch the webpage: Status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    page_text = ' '.join([para.get_text() for para in paragraphs])\n",
    "    return page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_length=512):\n",
    "    # Tokenize text and split into chunks of max_length tokens\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    chunks = []\n",
    "    for i in range(0, len(inputs[\"input_ids\"][0]), max_length):\n",
    "        chunk = tokenizer.decode(inputs[\"input_ids\"][0][i:i + max_length], skip_special_tokens=True)\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    summaries = [summarizer(chunk, max_length=500, min_length=30, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
    "    return ' '.join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentiment_analysis(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    return sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url):\n",
    "    # Fetch the webpage content\n",
    "    html_content = fetch_webpage(url)\n",
    "    \n",
    "    # Extract text from the HTML content\n",
    "    page_text = extract_text(html_content)\n",
    "    \n",
    "    # Summarize the text\n",
    "    summary = summarize_text(page_text)\n",
    "    \n",
    "    # Perform sentiment analysis on the summary\n",
    "    sentiment = perform_sentiment_analysis(summary)\n",
    "    \n",
    "    return {\n",
    "        'summary': summary,\n",
    "        'sentiment': sentiment\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: New Atlanticist is where top experts and policymakers offer exclusive insight on the most pressing global challenges. UkraineAlert provides regular news and analysis on developments in Ukraineâ€™s politics, economy, civil society, and culture. MENASource offers the latest news from across the Middle East.\n",
      "Sentiment: {'neg': 0.0, 'neu': 0.892, 'pos': 0.108, 'compound': 0.4336}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "url = 'https://www.atlanticcouncil.org/in-depth-research-reports/report/egypt-stability-gcc-priority/'  # Replace with the actual URL\n",
    "\n",
    "result = main(url)\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"Sentiment:\", result['sentiment'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
