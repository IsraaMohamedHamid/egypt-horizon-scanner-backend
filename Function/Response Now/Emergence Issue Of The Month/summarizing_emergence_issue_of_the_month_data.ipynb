{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient, UpdateOne, errors\n",
    "from cachetools import cached, TTLCache\n",
    "import logging\n",
    "from langdetect import detect, DetectorFactory\n",
    "from openai import OpenAI\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Ensure you have the VADER lexicon downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Ensure deterministic behavior in language detection\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Language mapping dictionary\n",
    "language_mapping = {\n",
    "    'en': 'English', 'ar': 'Arabic', 'fr': 'French', 'es': 'Spanish',\n",
    "    'de': 'German', 'zh': 'Chinese', 'ja': 'Japanese', 'ru': 'Russian',\n",
    "    'it': 'Italian', 'pt': 'Portuguese'\n",
    "}\n",
    "\n",
    "# MongoDB Connection\n",
    "def get_mongo_connection():\n",
    "    try:\n",
    "        mongo_uri = 'mongodb+srv://doadmin:w94yB2Y17dWE8C63@dbaas-db-5626135-310aba91.mongo.ondigitalocean.com/egypt-horizon-scanner?tls=true&authSource=admin&replicaSet=dbaas-db-5626135'\n",
    "        client = MongoClient(mongo_uri, tls=True, authSource='admin')\n",
    "        db = client[\"egypt-horizon-scanner\"]\n",
    "        client.admin.command('ping')  # Check connection\n",
    "        return db[\"emergence_issue_of_the_month_data\"]\n",
    "    except errors.ServerSelectionTimeoutError as err:\n",
    "        logging.error(\"Server selection timeout error: %s\", err)\n",
    "        sys.exit(1)\n",
    "    except errors.ConnectionFailure as err:\n",
    "        logging.error(\"Connection failure: %s\", err)\n",
    "        sys.exit(1)\n",
    "    except Exception as err:\n",
    "        logging.error(\"An unexpected error occurred: %s\", err)\n",
    "        sys.exit(1)\n",
    "\n",
    "# Cache configuration\n",
    "cache = TTLCache(maxsize=100, ttl=300)\n",
    "\n",
    "# OpenAI API client initialization\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Function to get response from GPT-3.5-turbo\n",
    "async def gpt_get(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #response_format={ \"type\": \"json_object\" }\n",
    "        )\n",
    "    return response.choices[0].message.content.strip(), response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "\n",
    "async def summarize_webpage(url):\n",
    "    prompt = f\"From the following link {url} Summarize the text in 5 lines.\"\n",
    "    summary, input_tokens, output_tokens = await gpt_get(prompt)\n",
    "    return summary, input_tokens, output_tokens\n",
    "\n",
    "async def sentiment_analysis(summary):\n",
    "    prompt = f\"Analyze the sentiment of the following text and return 'positive', 'neutral', or 'negative': {summary}\"\n",
    "    sentiment, input_tokens, output_tokens = await gpt_get(prompt)\n",
    "    return sentiment, input_tokens, output_tokens\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return language_mapping.get(detect(text), 'Unknown')\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in language detection: %s\", str(e))\n",
    "        return 'Unknown'\n",
    "\n",
    "def chunk_text(text, max_tokens=15000):\n",
    "    return [text[i:i + max_tokens] for i in range(0, len(text), max_tokens)]\n",
    "\n",
    "async def analyze_text(url):\n",
    "    summary, summary_input_tokens, summary_output_tokens = await summarize_webpage(url)\n",
    "    sentiment, sentiment_input_tokens, sentiment_output_tokens = await sentiment_analysis(summary)\n",
    "    \n",
    "    return summary.strip(), summary_input_tokens, summary_output_tokens, sentiment, sentiment_input_tokens, sentiment_output_tokens\n",
    "\n",
    "async def process_url(session, item):\n",
    "    try:\n",
    "        url = item.get('link')\n",
    "        if not url:\n",
    "            return {'_id': item.get('_id'), 'link': None, 'error': \"URL is missing\"}\n",
    "\n",
    "        summary, summary_input_tokens, summary_output_tokens, sentiment, sentiment_input_tokens, sentiment_output_tokens = await analyze_text(url)\n",
    "        lang = detect_language(summary)\n",
    "\n",
    "        result = {\n",
    "            '_id': item.get('_id'),\n",
    "            'emergingIssue': item.get('issueTitle'),\n",
    "            'language': lang,\n",
    "            'description': summary,\n",
    "            'summary_input_tokens': summary_input_tokens,\n",
    "            'summary_output_tokens': summary_output_tokens,\n",
    "            'sentimentAnalysis': sentiment,\n",
    "            'sentiment_input_tokens': sentiment_input_tokens,\n",
    "            'sentiment_output_tokens': sentiment_output_tokens\n",
    "        }\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error processing URL %s: %s\", item.get('link'), str(e))\n",
    "        return {'_id': item.get('_id'), 'link': item.get('link'), 'error': str(e)}\n",
    "\n",
    "async def update_emerging_issues_data(collection):\n",
    "    data = pd.DataFrame(list(collection.find()))\n",
    "\n",
    "    if data.empty:\n",
    "        logging.info(\"No data to process.\")\n",
    "        return\n",
    "\n",
    "    updates = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [process_url(session, row.to_dict()) for index, row in data.iterrows()]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "        for result in results:\n",
    "            if 'error' not in result:\n",
    "                row = data.loc[data['_id'] == result['_id']].iloc[0]\n",
    "                update_dict = {\n",
    "                    'filter': {\n",
    "                        '_id': row['_id'],\n",
    "                        'link': row['link'],\n",
    "                        'issueTitle': row['issueTitle'],\n",
    "                        'pillar': row['pillar'],\n",
    "                        'indicators': row['indicators'],\n",
    "                        'issuesMainSource': row['issuesMainSource'],\n",
    "                        'sourceCategory': row['sourceCategory']\n",
    "                    },\n",
    "                    'update': {'$set': result}\n",
    "                }\n",
    "                updates.append(update_dict)\n",
    "                total_input_tokens += result['summary_input_tokens'] + result['sentiment_input_tokens']\n",
    "                total_output_tokens += result['summary_output_tokens'] + result['sentiment_output_tokens']\n",
    "\n",
    "    logging.info(\"Processed %d records. Updating...\", len(data))\n",
    "\n",
    "    if updates:\n",
    "        try:\n",
    "            logging.info(\"Attempting to update %d records...\", len(updates))\n",
    "            result = collection.bulk_write([UpdateOne(x['filter'], x['update']) for x in updates], ordered=False)\n",
    "            logging.info(\"Successfully updated records.\")\n",
    "        except errors.BulkWriteError as e:\n",
    "            logging.error(\"Bulk write error: %s\", e.details)\n",
    "        except Exception as e:\n",
    "            logging.error(\"An error occurred during the update process: %s\", str(e))\n",
    "\n",
    "    # Calculate and print the total cost\n",
    "    input_cost = (total_input_tokens * 0.50) / 1_000_000\n",
    "    output_cost = (total_output_tokens * 1.50) / 1_000_000\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    logging.info(f\"Total cost: ${total_cost:.6f}\")\n",
    "\n",
    "def main():\n",
    "    db_collection = get_mongo_connection()\n",
    "\n",
    "    # Fetching all data from the collection\n",
    "    data = list(db_collection.find())\n",
    "\n",
    "    logging.info(f\"START: Total records: {len(data)}\")\n",
    "\n",
    "    # Validate URLs\n",
    "    valid_data = [item for item in data if isinstance(item, dict) and item.get('link')]\n",
    "\n",
    "    if not valid_data:\n",
    "        logging.info(\"No valid URLs found in the input data.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Attempt to process URLs\n",
    "    asyncio.run(update_emerging_issues_data(db_collection))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
